{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLWwgafyOoCMoAThWqLbt4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jayavarshini1711/Q-Gen/blob/main/QGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zaK9waue7V-"
      },
      "outputs": [],
      "source": [
        "!pip install -qU pdf4llm pymupdf transformers accelerate bitsandbytes tantivy gradio lancedb==0.20.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BE8n1G2yg0QB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries and Their Uses\n",
        "\n",
        "| Library/Tool        | Use Case in Questify                                                                       |\n",
        "|---------------------|----------------------------------------------------------------------|\n",
        "| pdf4llm           | Extracts structured content (text, images, tables) from PDFs         |\n",
        "| transformers      | Loads and runs LLMs (e.g., Mistral for question generation, BERT for classification) |\n",
        "| accelerate        | Speeds up model inference across GPU/CPU environments                |\n",
        "| bitsandbytes      | Enables low-bit quantization for memory-efficient LLMs               |\n",
        "| lancedb           | Stores SBERT embeddings for hybrid search of study content           |\n",
        "| tantivy           | Provides fast keyword-based full-text indexing and search            |\n",
        "\n",
        "\n",
        "| Model Name                     | Type                          | Layers | Max Seq Length | Use Case                              | Labels / Output      |\n",
        "|-------------------------------|-------------------------------|--------|----------------|----------------------------------------|-----------------------|\n",
        "| Mistral-7B-Instruct-v0.3      | MistralForCausalLM            | 32     | 32,768         | Question Generation                    | Text (Generated Qs)   |\n",
        "| all-MiniLM-L6-v2              | BertModel                     | 6      | 512            | Sentence Embeddings for Retrieval      | Embeddings            |\n",
        "| ms-marco-TinyBERT-L6          | BertForSequenceClassification | 6      | 512            | Passage Reranking                      | Relevance Score       |\n",
        "| cip29/blooms_bert             | BertForSequenceClassification | 12     | 512            | Bloom’s Taxonomy Classification        | 6 Bloom’s Levels      |\n"
      ],
      "metadata": {
        "id": "c-tzXyk93GTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d3XmX12Ygont"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertForSequenceClassification, BitsAndBytesConfig\n",
        "\n",
        "#Enable 4-bit Quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Use 4-bit precision\n",
        "    bnb_4bit_compute_dtype=\"float16\",  # Use float16 for faster computation\n",
        "    bnb_4bit_use_double_quant=True,  # Improves efficiency\n",
        "    bnb_4bit_quant_type=\"nf4\"  # NF4 quantization for better accuracy\n",
        ")\n",
        "\n",
        "#Load Tokenizer & Model with Quantization\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"cuda\"  # Automatically assigns model to GPU\n",
        ")\n",
        "\n",
        "#Load Bloom’s Taxonomy BERT Model\n",
        "blooms_model_name = \"cip29/blooms_bert\"\n",
        "blooms_tokenizer = BertTokenizer.from_pretrained(blooms_model_name)\n",
        "blooms_model = BertForSequenceClassification.from_pretrained(blooms_model_name, num_labels=6).to(\"cuda\")"
      ],
      "metadata": {
        "id": "jwV_TcMo3FNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9irPkCLwhmd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/*.pdf\n",
        "\n",
        "import pdf4llm\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import lancedb\n",
        "from lancedb.embeddings import get_registry\n",
        "from lancedb.pydantic import LanceModel, Vector\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Connect to LanceDB\n",
        "db = lancedb.connect(\"/content\")\n",
        "\n",
        "# Initialize SBERT Embedder\n",
        "embedder = get_registry().get(\"huggingface\").create(\n",
        "    name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "    device=\"cuda\"\n",
        ")\n",
        "\n",
        "# Load tokenizer to chunk text\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "class PDFSchema(LanceModel):\n",
        "    text: str = embedder.SourceField()              # chunk text (embedding input)\n",
        "    vector: Vector(embedder.ndims()) = embedder.VectorField()\n",
        "    page_name: str                                  # image/visual ID\n",
        "    full_text: str                                   # full page text for reference\n",
        "    page: int                                        # page number to detect duplicates\n",
        "\n",
        "\n",
        "\n",
        "# Upload PDFs\n",
        "uploaded = files.upload()\n",
        "print(list(uploaded.keys()))\n"
      ],
      "metadata": {
        "id": "Wkhrqz9E5jWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uV8KCRBGnEzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define chunking parameters\n",
        "CHUNK_SIZE = 480\n",
        "OVERLAP = 64\n",
        "\n",
        "# Function to process and chunk text with a sliding window\n",
        "def split_text_into_chunks(text, page_path, full_text, page_number):\n",
        "    input_ids = tokenizer.encode(text, truncation=False, add_special_tokens=False)\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(input_ids), CHUNK_SIZE - OVERLAP):\n",
        "        chunk_ids = input_ids[i:i + CHUNK_SIZE]\n",
        "\n",
        "        if len(chunk_ids) < 10:  # Skip very small chunks\n",
        "            continue\n",
        "\n",
        "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
        "        chunk_name = f\"{page_path}_chunk_{i // (CHUNK_SIZE - OVERLAP) + 1}\"\n",
        "\n",
        "        chunks.append({\n",
        "            \"text\": chunk_text,\n",
        "            \"page_name\": chunk_name,\n",
        "            \"full_text\": full_text,\n",
        "            \"page\": page_number\n",
        "        })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Collect all entries\n",
        "entries = []\n",
        "\n",
        "# Process each uploaded file\n",
        "uploaded_files = list(uploaded.keys())[:2]  # Limiting to 2 files for processing\n",
        "for pdf_filename in uploaded_files:\n",
        "    print(f\"\\nProcessing: {pdf_filename}\")\n",
        "\n",
        "    # Ask for page numbers or ranges\n",
        "    page_input = input(f\"Enter pages or ranges for {pdf_filename} (e.g., 1,3-5,7): \")\n",
        "\n",
        "    # Parse user input into zero-based page indices\n",
        "    selected_pages = []\n",
        "    for part in page_input.split(\",\"):\n",
        "        part = part.strip()\n",
        "        if \"-\" in part:\n",
        "            start, end = map(int, part.split(\"-\"))\n",
        "            selected_pages.extend(range(start - 1, end))  # Convert to zero-based index\n",
        "        else:\n",
        "            selected_pages.append(int(part) - 1)\n",
        "\n",
        "    # Extract specified pages\n",
        "    selected_page_data = pdf4llm.to_markdown(pdf_filename, page_chunks=True, pages=selected_pages)\n",
        "\n",
        "    # Process each page\n",
        "    for page_data in selected_page_data:\n",
        "        full_text = page_data[\"text\"]\n",
        "        page_path = page_data[\"metadata\"][\"file_path\"]\n",
        "        page_number = page_data[\"metadata\"][\"page\"]\n",
        "\n",
        "        if not full_text.strip():  # Skip empty pages\n",
        "            continue\n",
        "\n",
        "        # Split text into overlapping chunks with full page context\n",
        "        chunks = split_text_into_chunks(full_text, page_path, full_text, page_number)\n",
        "\n",
        "        # Add chunks to entries\n",
        "        entries.extend(chunks)\n",
        "\n",
        "# Store all entries in LanceDB\n",
        "tbl = db.create_table(\"pdf_data\", schema=PDFSchema, mode=\"overwrite\")\n",
        "tbl.add(entries)\n",
        "\n",
        "print(\"\\nAll selected pages have been chunked and stored in LanceDB with full page context!\")\n"
      ],
      "metadata": {
        "id": "nbhoPJzC5IeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FAQg1ewWoHL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lancedb.rerankers import CrossEncoderReranker\n",
        "\n",
        "# Initialize reranker\n",
        "reranker = CrossEncoderReranker()\n",
        "\n",
        "# User query\n",
        "query = input(\"\\nEnter your query: \")\n",
        "\n",
        "# Create full-text search index on the 'text' field\n",
        "tbl.create_fts_index(\"text\", replace=True)\n",
        "\n",
        "# Search and rerank\n",
        "results = tbl.search(query, query_type=\"hybrid\").rerank(reranker=reranker).limit(5).to_list()\n",
        "\n",
        "# Dictionary to hold unique pages\n",
        "unique_pages = {}\n",
        "\n",
        "# Filter out duplicates using the 'page' key\n",
        "for res in results:\n",
        "    page_number = res.get(\"page\")\n",
        "    if page_number not in unique_pages:\n",
        "        unique_pages[page_number] = res[\"full_text\"]\n",
        "\n",
        "# Final list of unique full_text values with page number\n",
        "final_full_texts = [{\"page\": page, \"text\": text} for page, text in unique_pages.items()]\n",
        "\n",
        "# Optional: Display them\n",
        "print(\"\\nUnique full_text entries by page:\\n\")\n",
        "for i, entry in enumerate(final_full_texts, 1):\n",
        "    print(f\"[{i}] Page {entry['page']}:\\n{entry['text'][:500]}...\\n{'-'*80}\")\n"
      ],
      "metadata": {
        "id": "Wu-ATHJs9_Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H2fh_JQ_orCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "\n",
        "# Bloom's Taxonomy Labels\n",
        "bloom_labels = {\n",
        "    0: \"BT1 (Remembering)\",\n",
        "    1: \"BT2 (Understanding)\",\n",
        "    2: \"BT3 (Applying)\",\n",
        "    3: \"BT4 (Analyzing)\",\n",
        "    4: \"BT5 (Evaluating)\",\n",
        "    5: \"BT6 (Creating)\"\n",
        "}\n",
        "\n",
        "# Utility functions\n",
        "def extract_numbered_list(text):\n",
        "    items = re.split(r'\\n(?=\\d+\\.\\s)', text.strip())\n",
        "    return [item.strip() for item in items if item.strip()]\n",
        "\n",
        "def split_number_and_text(item):\n",
        "    match = re.match(r'^(\\d+)\\.\\s+(.*)', item, re.DOTALL)\n",
        "    if match:\n",
        "        return int(match.group(1)), match.group(2).strip()\n",
        "    return None, item.strip()\n",
        "\n",
        "# Mistral Question Generation\n",
        "def generate_questions_with_mistral_bulk(pages_text, user_query):\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant specialized in question generation.\n",
        "Your goal is to generate insightful questions based on the given context and user query.\n",
        "\n",
        "Context:\n",
        "{pages_text}\n",
        "\n",
        "User Query (Focus Topic): {user_query}\n",
        "\n",
        "### Reasoning:\n",
        "- Step 1: Identify key points and concepts from the context relevant to the query\n",
        "- Step 2: Consider what types of questions best explore the topic of interest\n",
        "- Step 3: Formulate meaningful and topic-specific questions\n",
        "\n",
        "### Questions:\n",
        "\"\"\"\n",
        "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "    output = mistral_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=2048,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=mistral_tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = mistral_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text.split(\"### Questions:\")[-1].strip()\n",
        "\n",
        "# Mistral Answer Generation\n",
        "def generate_answer_key_with_mistral(questions_output, context):\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant specialized in answering technical questions.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Questions:\n",
        "{questions_output}\n",
        "\n",
        "### Answer Key:\n",
        "\"\"\"\n",
        "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "    output = mistral_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=4096,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=mistral_tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = mistral_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text.split(\"### Answer Key:\")[-1].strip()\n",
        "\n",
        "# Bloom’s Classifier\n",
        "def classify_blooms_taxonomy(question):\n",
        "    inputs = blooms_tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = blooms_model(**inputs)\n",
        "    probs = F.softmax(outputs.logits, dim=1).squeeze().tolist()\n",
        "    predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
        "    predicted_label = bloom_labels[predicted_idx]\n",
        "    prob_dict = {bloom_labels[i]: round(probs[i], 4) for i in range(6)}\n",
        "    return predicted_label, prob_dict\n",
        "\n",
        "\n",
        "\n",
        "def bulk_process_pages(pages_array, query, batch_size, output_csv=\"final_questions_classified.csv\"):\n",
        "    all_qa_pairs = []\n",
        "\n",
        "    for i in range(0, len(pages_array), batch_size):\n",
        "        batch_pages = pages_array[i:i+batch_size]\n",
        "        combined_text = \"\\n\".join(page[\"text\"] for page in batch_pages)\n",
        "        print(f\"\\n📄 Processing Pages {i+1} to {min(i+batch_size, len(pages_array))}...\")\n",
        "\n",
        "        # Generate questions and answers\n",
        "        questions = generate_questions_with_mistral_bulk(combined_text, query)\n",
        "        answers = generate_answer_key_with_mistral(questions_output=questions, context=combined_text)\n",
        "\n",
        "        question_items = extract_numbered_list(questions)\n",
        "        answer_items = extract_numbered_list(answers)\n",
        "\n",
        "        for q_item, a_item in zip(question_items, answer_items):\n",
        "            q_num, q_text = split_number_and_text(q_item)\n",
        "            a_num, a_text = split_number_and_text(a_item)\n",
        "            if q_num == a_num:\n",
        "                all_qa_pairs.append((q_num, q_text, a_text))\n",
        "            else:\n",
        "                print(f\"⚠️ Mismatch: Question {q_num} doesn't match Answer {a_num}\")\n",
        "\n",
        "    # Write classified Q&A to single CSV\n",
        "    with open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            \"No\", \"Question\",\"Bloom's Taxonomy Level\", \"Answer\",\n",
        "            \"BT1 (Remembering)\", \"BT2 (Understanding)\", \"BT3 (Applying)\",\n",
        "            \"BT4 (Analyzing)\", \"BT5 (Evaluating)\", \"BT6 (Creating)\"\n",
        "        ])\n",
        "\n",
        "        for idx, (num, question, answer) in enumerate(all_qa_pairs, 1):\n",
        "            bloom_level, probs = classify_blooms_taxonomy(question)\n",
        "            writer.writerow([\n",
        "                idx, question, bloom_level, answer,\n",
        "                probs[\"BT1 (Remembering)\"], probs[\"BT2 (Understanding)\"],\n",
        "                probs[\"BT3 (Applying)\"], probs[\"BT4 (Analyzing)\"],\n",
        "                probs[\"BT5 (Evaluating)\"], probs[\"BT6 (Creating)\"]\n",
        "            ])\n",
        "\n",
        "    print(f\"\\n✅ Saved {len(all_qa_pairs)} classified Q&A pairs to '{output_csv}'\")\n",
        "    files.download(output_csv)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C5_hP25kw6LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GbCjw1QPou1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = input(\"\\nEnter your query: \")\n",
        "bulk_process_pages(final_full_texts, query, batch_size=5)"
      ],
      "metadata": {
        "id": "x_75Y3UlDMD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_BxqKHdzfDN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}